{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItVto8BntzJu"
   },
   "source": [
    "<h1 align=center><font size = 4>HW2: SVM,Naive Bayes and Decision Trees for Sentiment Classification</font></h1> \n",
    "<h2 align=center><font size = 3>【Machine Learning Course】</font></h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GopaO5S8uJi1"
   },
   "source": [
    "- This is an individual assignment. However, you are allowed to discuss the problems with other students in the class. But you should write your own code and report.\n",
    "- If you have any discussion with others, you should acknowledge the discussion in your report by mentioning their name.\n",
    "- All the hyper-parameter details and other reporting details must be in the pdf report file only. <font color=red>You will have three types of files, 1) a single report (pdf), 2) dataset files (.txt), 3) a single code file (.ipynb). You should submit two files, one pdf file with report (<your-student-ID>.pdf) and one zip file with the dataset after preprocessing and codes (<your-student-ID>.zip)</font>. For the dataset format, please refer to the end of this notebook.\n",
    "- You are free to use libraries with general utilities, such as matplotlib, numpy and scipy for python. You can use pre-existing implementation of the algorithms available in scikit-learn package. Do not use NLTK or any other NLP libraries for pre-processing.\n",
    "## Sentiment Classification\n",
    "In this assignment, we will design a sentiment classifier for classifying the sentiment of the reviews. This is a Natural Language Processing (NLP) task where the input is a natural language text and output is the sentiment label. We will consider the dataset yelp reviews for restaurants.\n",
    "## Yelp dataset\n",
    "The Yelp dataset consists of 7000 reviews in the training set, 1000 reviews in the validation set, and 2000 reviews in the test set. This is a 5 class problem where each review is classied into one of the five ratings with rating-5 being the best score and rating-1 being the worst score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MocvM7BXuZdE"
   },
   "source": [
    "\n",
    "<h1 align=center><font size = 4>实践学习部分</font></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D54NCc_GuV0E"
   },
   "source": [
    "### 步骤1 引入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A1Tfx4a1c4Mk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v41wmvQRui-D"
   },
   "source": [
    "### 步骤2 数据预处理\n",
    "Most of the algorithms described in the class expects input as a vector. However, the\n",
    "reviews are natural language text of varying number of words. So the first step would\n",
    "be to convert this varying length movie review to a fixed length vector representation.\n",
    "We will consider two different ways of vectorizing the natural language text: binary\n",
    "bag-of-words representation and frequency bag-of-words representation (as explained in\n",
    "the end of the assignment). Convert both the datasets into both these representations.\n",
    "Instruction for dataset submission is given in the end of the assignment ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NwZpV3O3Pet7"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# This is to find the frequency map\n",
    "# \n",
    "train_path = './yelp-train.txt'\n",
    "with open(train_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "dic = defaultdict(list)\n",
    "for line in lines:\n",
    "    sentence = line.split('\\t')[0]\n",
    "    sentiment = line.split('\\t')[1][0]\n",
    "    dic['sentence'].append(sentence)\n",
    "    dic['sentiment'].append(sentiment)\n",
    "                            \n",
    "df = pd.DataFrame(dic)\n",
    "# this is to punctuate the words\n",
    "sets = set()\n",
    "for s in df['sentence']:\n",
    "    for word in s.split():\n",
    "        sets.add(word.lower())\n",
    "        \n",
    "ls = list(sets)\n",
    "punctuation = ['.',',',':',')','(','\"',\"'\",'/','!','~','+','-','?','*']\n",
    "for punc in punctuation:\n",
    "    ls = list(sets)\n",
    "    sets = set()\n",
    "    for word in ls:\n",
    "        for w in word.split(punc):\n",
    "            if w:\n",
    "                sets.add(w)\n",
    "# this is to lowercase them and remove all not-word word\n",
    "ls = list(sets)\n",
    "sets = set()\n",
    "for word in ls:\n",
    "    flag = True\n",
    "    word = word.lower()\n",
    "    for w in word:\n",
    "        if w < 'a' or w > 'z':\n",
    "            flag = False\n",
    "            break\n",
    "    if flag:\n",
    "        sets.add(word)\n",
    "        \n",
    "# calculate the frequency\n",
    "newdic = defaultdict(int)\n",
    "for s in df['sentence']:\n",
    "    for word in s.split():\n",
    "        if word in sets:\n",
    "            newdic[word] += 1\n",
    "newdic = dict(newdic)\n",
    "so = list(newdic.items())\n",
    "so.sort(key = lambda x:x[1], reverse=True)\n",
    "\n",
    "\n",
    "# print the dictionary\n",
    "so_list = list(so)\n",
    "so_list = [list(item) for item in so_list]\n",
    "so_list = list(enumerate(so_list,1))\n",
    "#so_list = [list(item) for item in so_list]\n",
    "output = list()\n",
    "\n",
    "for i in range(0,len(so_list)):\n",
    "    ite = list()\n",
    "    ite.append(so_list[i][1][0])\n",
    "    ite.append(so_list[i][0])\n",
    "    ite.append(so_list[i][1][1])\n",
    "    output.append(ite)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成词表 以下直至步骤三不建议重新运行，很慢，且不运行不影响后续分析\n",
    "data=open(\"./yelp-vocab1.txt\",'w+') \n",
    "for i in range(0,10000):\n",
    "    print(output[i],file=data)\n",
    "data.close()\n",
    "\n",
    "path = './yelp-vocab1.txt'\n",
    "with open(path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "data=open(\"./yelp-vocab.txt\",'w+')\n",
    "for line in lines:\n",
    "    line=line.replace(\",\",\"\\t\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")#改变元素\n",
    "    print(line, file = data)\n",
    "data.close()\n",
    "#os.remove(r'./yelp-vocab1.txt') 这句话加在所有预处理完事儿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成train的替换后的文档\n",
    "train_path = './yelp-train1.txt'\n",
    "with open(train_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "dic = defaultdict(list)\n",
    "for line in lines:\n",
    "    sentence = line.split('\\t')[0]\n",
    "    sentiment = line.split('\\t')[1][0]\n",
    "    dic['sentence'].append(sentence)\n",
    "    dic['sentiment'].append(sentiment)\n",
    "                            \n",
    "df = pd.DataFrame(dic)\n",
    "#os.remove(r'./yelp-train.txt')\n",
    "for j in range(0,7000):\n",
    "    words = list()\n",
    "    for word in df['sentence'][j].split():\n",
    "        words.append(word.lower())\n",
    "    for word in range(0,len(words)):\n",
    "        for i in range(0,10000):\n",
    "            if words[word].lower() == so_list[i][1][0]:\n",
    "                words[word] = str(so_list[i][0])\n",
    "\n",
    "    data=open(\"./yelp-train.txt\",'a')\n",
    "    for word in words:\n",
    "        word=word.replace(\",\",\" \").replace(\"[\",\"\").replace(\"]\",\"\").replace('\"',\"\")#改变元素\n",
    "        print(word, file = data, end = \" \")\n",
    "    print('\\t',df['sentiment'][j], file = data)\n",
    "    data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成valid的替换后的文档\n",
    "valid_path = './yelp-valid1.txt'\n",
    "with open(valid_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "dic = defaultdict(list)\n",
    "for line in lines:\n",
    "    sentence = line.split('\\t')[0]\n",
    "    sentiment = line.split('\\t')[1][0]\n",
    "    dic['sentence'].append(sentence)\n",
    "    dic['sentiment'].append(sentiment)\n",
    "#os.remove(r'./yelp-vaild.txt')                            \n",
    "df = pd.DataFrame(dic)\n",
    "for j in range(0,1000):\n",
    "    words = list()\n",
    "    for word in df['sentence'][j].split():\n",
    "        words.append(word.lower())\n",
    "    for word in range(0,len(words)):\n",
    "        for i in range(0,10000):\n",
    "            if words[word].lower() == so_list[i][1][0]:\n",
    "                words[word] = str(so_list[i][0])\n",
    "\n",
    "    data=open(\"./yelp-valid.txt\",'a')\n",
    "    for word in words:\n",
    "        word=word.replace(\",\",\" \").replace(\"[\",\"\").replace(\"]\",\"\").replace('\"',\"\")#改变元素\n",
    "        print(word, file = data, end = \" \")\n",
    "    print('\\t',df['sentiment'][j], file = data)\n",
    "    data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成test的替换后的文档\n",
    "test_path = './yelp-test1.txt'\n",
    "with open(test_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "dic = defaultdict(list)\n",
    "for line in lines:\n",
    "    sentence = line.split('\\t')[0]\n",
    "    sentiment = line.split('\\t')[1][0]\n",
    "    dic['sentence'].append(sentence)\n",
    "    dic['sentiment'].append(sentiment)\n",
    "#os.remove(r'./yelp-vaild.txt')                            \n",
    "df = pd.DataFrame(dic)\n",
    "for j in range(0,2000):\n",
    "    words = list()\n",
    "    for word in df['sentence'][j].split():\n",
    "        words.append(word.lower())\n",
    "    for word in range(0,len(words)):\n",
    "        for i in range(0,10000):\n",
    "            if words[word].lower() == so_list[i][1][0]:\n",
    "                words[word] = str(so_list[i][0])\n",
    "\n",
    "    data=open(\"./yelp-test.txt\",'a')\n",
    "    for word in words:\n",
    "        word=word.replace(\",\",\" \").replace(\"[\",\"\").replace(\"]\",\"\").replace('\"',\"\")#改变元素\n",
    "        print(word, file = data, end = \" \")\n",
    "    print('\\t',df['sentiment'][j], file = data)\n",
    "    data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sE2odoTGupcT"
   },
   "source": [
    "### 步骤3 yelp数据集：用BBOW构建模型\n",
    "For this question, we will focus on the yelp dataset with binary bag-of-words (BBoW)\n",
    "representation. We will use the F1-measure as the evaluation metric for the entire\n",
    "assignment.\n",
    "\n",
    "(a) As a baseline, report the performance of the random classifier (a classifier which\n",
    "classifies a review into an uniformly random class) and the majority-class classifier\n",
    "(a classifier which computes the majority class in the training set and classifies all\n",
    "test instances as that majority class).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XSqDQ1h2R3aP"
   },
   "outputs": [],
   "source": [
    "# 请在这里填写您的代码\n",
    "# BBOW数据预处理\n",
    "def bbowpreprocessing(dic):\n",
    "    file_handle=open(dic,mode='r')\n",
    "    content0=file_handle.readlines()\n",
    "    #属性\n",
    "    words_list=[[0 for col in range(10000)] for row in range(len(content0))]\n",
    "    #目标\n",
    "    sentiment=[0 for row in range(len(content0))]\n",
    "\n",
    "    x1=''\n",
    "    for i in range(len(content0)):\n",
    "        sentiment[i]=content0[i][-2]\n",
    "        for j in range(len(content0[i])-1):\n",
    "            if content0[i][j].isdigit():\n",
    "                x1+=content0[i][j]\n",
    "            else:\n",
    "                if x1!='':\n",
    "                    x1=int(x1)\n",
    "                    if 0<=x1<10000:\n",
    "                        words_list[i][x1-1]=1\n",
    "                    x1=''\n",
    "    return words_list, sentiment\n",
    "\n",
    "\n",
    "words_list_train, sentiment_train = bbowpreprocessing('./yelp-train.txt')\n",
    "words_list_valid, sentiment_valid = bbowpreprocessing('./yelp-valid.txt')\n",
    "words_list_test, sentiment_test = bbowpreprocessing('./yelp-test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机分类器test的f1= 0.20739646641749476\n",
      "随机分类器train的f1= 0.21261962746532156\n",
      "随机分类器train的f1= 0.1903313901283753\n"
     ]
    }
   ],
   "source": [
    "# 随机分类器\n",
    "randomclass_test = list()\n",
    "for i in range(2000):\n",
    "    randomclass_test.append(int(np.random.rand()*5)+1)\n",
    "    \n",
    "sentiment_test = [int(i) for i in sentiment_test]\n",
    "f1 = f1_score(sentiment_test, randomclass_test, average = 'weighted')\n",
    "\n",
    "print('随机分类器test的f1=',f1)\n",
    "\n",
    "randomclass_train = list()\n",
    "for i in range(7000):\n",
    "    randomclass_train.append(int(np.random.rand()*5)+1)\n",
    "    \n",
    "sentiment_train = [int(i) for i in sentiment_train]\n",
    "f1 = f1_score(sentiment_train, randomclass_train, average = 'weighted')\n",
    "\n",
    "print('随机分类器train的f1=',f1)\n",
    "\n",
    "randomclass_valid = list()\n",
    "for i in range(1000):\n",
    "    randomclass_valid.append(int(np.random.rand()*5)+1)\n",
    "    \n",
    "sentiment_valid = [int(i) for i in sentiment_valid]\n",
    "f1 = f1_score(sentiment_valid, randomclass_valid, average = 'weighted')\n",
    "\n",
    "print('随机分类器train的f1=',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大类分类器test的f1= 0.18238490007401925\n",
      "大类分类器train的f1= 0.18380783390669325\n",
      "大类分类器valid的f1= 0.18692625368731566\n"
     ]
    }
   ],
   "source": [
    "# 大类分类器\n",
    "result = pd.value_counts(sentiment_train)\n",
    "majorityclass = list()\n",
    "for i in range(2000):\n",
    "    majorityclass.append(result.index[0])\n",
    "\n",
    "majorityclass = [int(i) for i in majorityclass]\n",
    "f1 = f1_score(sentiment_test, majorityclass, average = 'weighted')\n",
    "print('大类分类器test的f1=',f1)\n",
    "\n",
    "result = pd.value_counts(sentiment_train)\n",
    "majorityclass = list()\n",
    "for i in range(7000):\n",
    "    majorityclass.append(result.index[0])\n",
    "\n",
    "majorityclass = [int(i) for i in majorityclass]\n",
    "f1 = f1_score(sentiment_train, majorityclass, average = 'weighted')\n",
    "print('大类分类器train的f1=',f1)\n",
    "\n",
    "result = pd.value_counts(sentiment_train)\n",
    "majorityclass = list()\n",
    "for i in range(1000):\n",
    "    majorityclass.append(result.index[0])\n",
    "\n",
    "majorityclass = [int(i) for i in majorityclass]\n",
    "f1 = f1_score(sentiment_valid, majorityclass, average = 'weighted')\n",
    "print('大类分类器valid的f1=',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIWyMpi-uvfa"
   },
   "source": [
    "# (b) Now train Naive Bayes, Decision Trees, and Linear SVM for this task. \n",
    "[Note: You\n",
    "should do a thorough hyper-parameter tuning by using the given validation set.\n",
    "Also, note that you should use the appropriate naive Bayes classifier for binary\n",
    "input features (also called Bernoulli naive Bayes).]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yMu6DB3RScVm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 请在这里填写您的代码 (Naive Bayes)\n",
    "Bern = BernoulliNB()\n",
    "Bern.fit(words_list_train, sentiment_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "伯努利朴素贝叶斯test的f1= 0.3914862429730039\n",
      "伯努利朴素贝叶斯train的f1= 0.5840324991638591\n",
      "伯努利朴素贝叶斯valid的f1= 0.3643033176913067\n"
     ]
    }
   ],
   "source": [
    "test_outcome_bern = Bern.predict(words_list_test)\n",
    "test_outcome_bern = [int(i) for i in test_outcome_bern]\n",
    "f1 = f1_score(sentiment_test, test_outcome_bern, average = 'weighted')\n",
    "print('伯努利朴素贝叶斯test的f1=',f1)\n",
    "test_outcome_bern = Bern.predict(words_list_train)\n",
    "test_outcome_bern = [int(i) for i in test_outcome_bern]\n",
    "f1 = f1_score(sentiment_train, test_outcome_bern, average = 'weighted')\n",
    "print('伯努利朴素贝叶斯train的f1=',f1)\n",
    "test_outcome_bern = Bern.predict(words_list_valid)\n",
    "test_outcome_bern = [int(i) for i in test_outcome_bern]\n",
    "f1 = f1_score(sentiment_valid, test_outcome_bern, average = 'weighted')\n",
    "print('伯努利朴素贝叶斯valid的f1=',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5uoqGWcsuzli"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best patams is{'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 12, 'random_state': 1, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# 请在这里填写您的代码 ( Decision Trees)\n",
    "# 寻找最优参数\n",
    "depth = list()\n",
    "for i in range(5,13):\n",
    "    depth.append(i)\n",
    "DTC = GridSearchCV(DecisionTreeClassifier(), param_grid={\"class_weight\": [\"balanced\"], \"criterion\": (\"gini\", 'entropy'),\\\n",
    "      \"splitter\": ['random', 'best'], \"max_depth\": depth, 'random_state': [1]})\n",
    "DTC.fit(words_list_train, sentiment_train)\n",
    "validpred = DTC.predict(words_list_valid)\n",
    "#sigma = metrics.mean_squared_error(sentiment_valid, validpred) #计算均方差\n",
    "#print(sigma)\n",
    "print('the best patams is{}'.format(DTC.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight='balanced', criterion='gini',\n",
       "                       max_depth=12, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=1, splitter='best')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#最优参数代入\n",
    "DTC = DecisionTreeClassifier(criterion='gini', max_depth=12, class_weight = 'balanced', splitter = 'best', random_state = 1)\n",
    "DTC.fit(words_list_train, sentiment_train)\n",
    "# test_outcome_bern = [int(i) for i in test_outcome_bern]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树test的f1= 0.31142727259250935\n",
      "决策树train的f1= 0.47581208772663364\n",
      "决策树valid的f1= 0.3023970557648267\n"
     ]
    }
   ],
   "source": [
    "DTC = DecisionTreeClassifier(criterion='gini', max_depth=12, class_weight = 'balanced', splitter = 'best', random_state = 1)\n",
    "DTC.fit(words_list_train, sentiment_train)\n",
    "test_outcome_bern_train = DTC.predict(words_list_train)\n",
    "test_outcome_bern_train = [int(i) for i in test_outcome_bern_train]\n",
    "f1 = f1_score(sentiment_train, test_outcome_bern_train, average = 'weighted')\n",
    "print('决策树train的f1=',f1)\n",
    "test_outcome_bern_valid = DTC.predict(words_list_valid)\n",
    "test_outcome_bern_valid = [int(i) for i in test_outcome_bern_valid]\n",
    "f1 = f1_score(sentiment_valid, test_outcome_bern_valid, average = 'weighted')\n",
    "print('决策树valid的f1=',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "z6q1Dna3u2B7"
   },
   "outputs": [],
   "source": [
    "# 请在这里填写您的代码 ( Linear SVM)\n",
    "# 寻找最优参数\n",
    "svr = GridSearchCV(LinearSVC(), param_grid={\"C\": np.logspace(-3, 3, 7)})\n",
    "\n",
    "svr.fit(words_list_train, sentiment_train) #训练\n",
    "sentiment_pred = svr.predict(words_list_valid)   #测试集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best parameter is{'C': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print('the best parameter is{}'.format(svr.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#最优参数代入\n",
    "LSVC = LinearSVC(C = 0.01)\n",
    "LSVC.fit(words_list_train, sentiment_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性支持向量机test的f1= 0.4710937321447393\n",
      "线性支持向量机train的f1= 0.8065874229852863\n",
      "线性支持向量机valid的f1= 0.45023726619464627\n"
     ]
    }
   ],
   "source": [
    "test_outcome_LSVC = LSVC.predict(words_list_test)\n",
    "#test_outcome_LSVC = [int(i) for i in test_outcome_LSVC]\n",
    "f1 = f1_score(sentiment_test, test_outcome_LSVC, average = 'weighted')\n",
    "print('线性支持向量机test的f1=',f1)\n",
    "test_outcome_LSVC = LSVC.predict(words_list_train)\n",
    "#test_outcome_LSVC = [int(i) for i in test_outcome_LSVC]\n",
    "f1 = f1_score(sentiment_train, test_outcome_LSVC, average = 'weighted')\n",
    "print('线性支持向量机train的f1=',f1)\n",
    "test_outcome_LSVC = LSVC.predict(words_list_valid)\n",
    "#test_outcome_LSVC = [int(i) for i in test_outcome_LSVC]\n",
    "f1 = f1_score(sentiment_valid, test_outcome_LSVC, average = 'weighted')\n",
    "print('线性支持向量机valid的f1=',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yj0IPgtZu4YK"
   },
   "source": [
    "(c) Report the list of hyper-parameters you considered for each classifier, the range of the individual hyper-parameters and the best value for these hyper-parameters chosen based on the validation set performance1.\n",
    "\n",
    "(d) Report training, validation, and test F1-measure for all the classifiers (with best hyper-parameter configuration).\n",
    "\n",
    "(e) Comment about the performance of different classifiers. Why did a particular\n",
    "classifier performed better than the others? What was the role of that hyper-\n",
    "parameter that fetched you the best results.\n",
    "\n",
    "### 步骤4 yelp数据集：用FBoW构建模型\n",
    "Now we will repeat step 3 but with frequency bag-of-words (FBoW) representation.\n",
    "\n",
    "(a) Train Naive Bayes, Decision Trees, and Linear SVM for this task. [Note: You\n",
    "should do a thorough hyper-parameter tuning by using the given validation set. Also, note that you should use the appropriate naive Bayes classifier for real valued\n",
    "input features (also called Gaussian naive Bayes).]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请在这里填写您的代码\n",
    "# FBOW数据预处理\n",
    "def fbowpreprocessing(dic):\n",
    "    file_handle=open(dic,mode='r')\n",
    "    content0=file_handle.readlines()\n",
    "    #属性\n",
    "    words_list=[[0 for col in range(10000)] for row in range(len(content0))]\n",
    "    #目标\n",
    "    sentiment=[0 for row in range(len(content0))]\n",
    "\n",
    "    x1=''\n",
    "    for i in range(len(content0)):\n",
    "        sentiment[i]=content0[i][-2]\n",
    "        for j in range(len(content0[i])-1):\n",
    "            if content0[i][j].isdigit():\n",
    "                x1+=content0[i][j]\n",
    "            else:\n",
    "                if x1!='':\n",
    "                    x1=int(x1)\n",
    "                    if 0<=x1<10000:\n",
    "                        words_list[i][x1-1]+=1\n",
    "                    x1=''\n",
    "    sum = 0\n",
    "    for i in range(10000):\n",
    "        sum += words_list[0][i]\n",
    "    for i in range(10000):\n",
    "        words_list[0][i] = words_list[0][i]/sum\n",
    "    return words_list, sentiment\n",
    "\n",
    "words_list_train_fbow, sentiment_train_fbow = fbowpreprocessing('./yelp-train.txt')\n",
    "words_list_valid_fbow, sentiment_valid_fbow = fbowpreprocessing('./yelp-valid.txt')\n",
    "words_list_test_fbow, sentiment_test_fbow = fbowpreprocessing('./yelp-test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rMF89fj1vGlS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 请在这里填写您的代码 ( Naive Bayes)\n",
    "Gau = GaussianNB()\n",
    "Gau.fit(words_list_train_fbow, sentiment_train_fbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高斯朴素贝叶斯test的f1= 0.2760047772674479\n",
      "高斯朴素贝叶斯train的f1= 0.6651035134883894\n",
      "高斯朴素贝叶斯valid的f1= 0.25636765458952404\n"
     ]
    }
   ],
   "source": [
    "test_outcome_gau = Gau.predict(words_list_test_fbow)\n",
    "#test_outcome_gau = [int(i) for i in test_outcome_gau]\n",
    "f1 = f1_score(sentiment_test_fbow, test_outcome_gau, average = 'weighted')\n",
    "print('高斯朴素贝叶斯test的f1=',f1)\n",
    "test_outcome_gau = Gau.predict(words_list_train_fbow)\n",
    "#test_outcome_gau = [int(i) for i in test_outcome_gau]\n",
    "f1 = f1_score(sentiment_train_fbow, test_outcome_gau, average = 'weighted')\n",
    "print('高斯朴素贝叶斯train的f1=',f1)\n",
    "test_outcome_gau = Gau.predict(words_list_valid_fbow)\n",
    "#test_outcome_gau = [int(i) for i in test_outcome_gau]\n",
    "f1 = f1_score(sentiment_valid_fbow, test_outcome_gau, average = 'weighted')\n",
    "print('高斯朴素贝叶斯valid的f1=',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Mvm714BmvIyq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best parameters are{'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 7, 'random_state': 1, 'splitter': 'random'}\n"
     ]
    }
   ],
   "source": [
    "# 请在这里填写您的代码 ( Decision Trees)\n",
    "# 寻找最优参数\n",
    "depth = list()\n",
    "for i in range(5,13):\n",
    "    depth.append(i)\n",
    "DTC = GridSearchCV(DecisionTreeClassifier(), param_grid={\"class_weight\": [\"balanced\"], \"criterion\": (\"gini\", 'entropy'),\\\n",
    "      \"splitter\": ['random','best'], \"max_depth\": depth, 'random_state': [1]})\n",
    "DTC.fit(words_list_train_fbow, sentiment_train_fbow)\n",
    "validpred = DTC.predict(words_list_valid_fbow)\n",
    "#sigma = metrics.mean_squared_error(sentiment_valid_fbow, validpred) #计算均方差\n",
    "#print(sigma)\n",
    "print('the best parameters are{}'.format(DTC.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight='balanced', criterion='gini',\n",
       "                       max_depth=7, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=1, splitter='random')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#最优参数代入\n",
    "DTC = DecisionTreeClassifier(criterion='gini', max_depth=7, class_weight = 'balanced', splitter = 'random', random_state = 1)\n",
    "DTC.fit(words_list_train_fbow, sentiment_train_fbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树test的f1= 0.33034472621680333\n",
      "决策树train的f1= 0.37573213516397896\n",
      "决策树valid的f1= 0.3262174480754342\n"
     ]
    }
   ],
   "source": [
    "test_outcome = DTC.predict(words_list_test_fbow)\n",
    "# test_outcome = [int(i) for i in test_outcome]\n",
    "f1 = f1_score(sentiment_test_fbow, test_outcome, average = 'weighted')\n",
    "print('决策树test的f1=',f1)\n",
    "test_outcome = DTC.predict(words_list_train_fbow)\n",
    "# test_outcome = [int(i) for i in test_outcome]\n",
    "f1 = f1_score(sentiment_train_fbow, test_outcome, average = 'weighted')\n",
    "print('决策树train的f1=',f1)\n",
    "test_outcome = DTC.predict(words_list_valid_fbow)\n",
    "# test_outcome = [int(i) for i in test_outcome]\n",
    "f1 = f1_score(sentiment_valid_fbow, test_outcome, average = 'weighted')\n",
    "print('决策树valid的f1=',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "MVB9btZavK5i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best parameter is{'C': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# 请在这里填写您的代码 ( Linear SVM)\n",
    "# 寻找最优参数\n",
    "svr = GridSearchCV(LinearSVC(), param_grid={\"C\": np.logspace(-3, 3, 7)})\n",
    "\n",
    "svr.fit(words_list_train_fbow, sentiment_train_fbow) #训练\n",
    "sentiment_pred = svr.predict(words_list_valid_fbow)   #测试集测试\n",
    "print('the best parameter is{}'.format(svr.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#最优参数代入\n",
    "LSVC = LinearSVC(C = 0.01)\n",
    "LSVC.fit(words_list_train_fbow, sentiment_train_fbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性支持向量机test的f1= 0.46979351639773886\n",
      "线性支持向量机train的f1= 0.8131439860094576\n",
      "线性支持向量机valid的f1= 0.4623409504195169\n"
     ]
    }
   ],
   "source": [
    "test_outcome_LSVC = LSVC.predict(words_list_test_fbow)\n",
    "#test_outcome_LSVC = [int(i) for i in test_outcome_LSVC]\n",
    "f1 = f1_score(sentiment_test_fbow, test_outcome_LSVC, average = 'weighted')\n",
    "print('线性支持向量机test的f1=',f1)\n",
    "test_outcome_LSVC = LSVC.predict(words_list_train_fbow)\n",
    "#test_outcome_LSVC = [int(i) for i in test_outcome_LSVC]\n",
    "f1 = f1_score(sentiment_train_fbow, test_outcome_LSVC, average = 'weighted')\n",
    "print('线性支持向量机train的f1=',f1)\n",
    "test_outcome_LSVC = LSVC.predict(words_list_valid_fbow)\n",
    "#test_outcome_LSVC = [int(i) for i in test_outcome_LSVC]\n",
    "f1 = f1_score(sentiment_valid_fbow, test_outcome_LSVC, average = 'weighted')\n",
    "print('线性支持向量机valid的f1=',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVnMprVpvNUK"
   },
   "source": [
    "(b) Report the list of hyper-parameters you considered for each classifier, the range of the individual hyper-parameters and the best value for these hyper-parameters\n",
    "chosen based on the validation set performance.\n",
    "\n",
    "(c) Report training, validation, and test F1-measure for all the classifiers (with best hyper-parameter configuration).\n",
    "\n",
    "(d) Comment about the performance of different classifiers. Why did a particular\n",
    "classifier performed better than the others? What was the role of that hyper-\n",
    "parameter that fetched you the best results.\n",
    "\n",
    "(e) Compare the performance with the binary bag-of-words based classifiers. Why the\n",
    "difference in performance? Give a brief explanation comparing BBoW Naive Bayes\n",
    "and FBoW Naive Bayes and similarly for Decision Trees and Linear SVM.\n",
    "\n",
    "(f) Which representation is better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEsak1OTvXvh"
   },
   "source": [
    "\n",
    "<h1 align=center><font size = 4>实验补充说明</font></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWDWkdwAvbDx"
   },
   "source": [
    "## Instruction for binary bag-of-words representation\n",
    "1. First step is to construct the word vocabulary for a dataset. Given a dataset, we\n",
    "will only consider the training set and enumerate all unique words in the training\n",
    "set reviews. To do this, we should do a bit of pre-processing to the raw text. For\n",
    "this assignment, we will do only 2 pre-processing steps: removal of punctuation, and\n",
    "lower-casing the words. For example, (How old are you?) would become (how old are\n",
    "you).\n",
    "2. Once you have the vocabulary, count the frequency of each word in the training set\n",
    "and sort the words in the vocabulary based on frequency (in descending order). Now\n",
    "pick top 10,000 words in the vocabulary and ignore the rest of the words. These 10,000\n",
    "words will form the feature set for our classification process.\n",
    "3. For any example to be classified, generate a 10,000 dimensional feature vector as follows: for each of the top 10,000 words, there is one corresponding dimension in the\n",
    "feature vector that is 1 if the example contains the word, and 0 otherwise. Make sure\n",
    "to use the same pre-processing steps as before for validation and test reviews.\n",
    "\n",
    "## Instruction for frequency bag-of-words representation\n",
    "1. Follow steps 1-2 of the instructions for the binary bag-of-words.\n",
    "2. Now, for each of the 10,000 words, the corresponding feature is the frequency of occurrence of that word in the given review. You can calculate this by summing the\n",
    "occurrences of words in a review into a histogram, and than divide by the sum of\n",
    "occurrences of all 10,000 words so that the vector for each example sums to 1.\n",
    "\n",
    "## Instruction for dataset submission\n",
    "You should submit yelp-train.txt, yelp-valid.txt, yelp-test.txt,yelp-vocab.txt.\n",
    "1. yelp-vocab.txt file should contain the 10,000 words in the vocabulary, their corresponding id (starting from 1), and their frequency. Each line is a word, its numeric id,\n",
    "and its frequency all tab separated. Example:  \n",
    "the 1 20456  \n",
    "where the is the word, 1 is the id of the word, and 20456 is the frequency of the\n",
    "word.\n",
    "2. For train/valid/test file, each line is a data point. Each review should be represented\n",
    "as space separated ids for corresponding words in the review and the class label in\n",
    "mentioned in the end of the review as tab separated. Skip ids for words which are not\n",
    "there in top 10000. Just replace the word with the id (don't sort the ids in a row).\n",
    "Example:  \n",
    "100 8 3 1034 0   \n",
    "Here 0 is the class label and rest of the numbers represent a 4 word review.\n",
    "\n",
    "## Instruction for code submission\n",
    "1. Submit a single zipped folder with your student id as the name of the folder. For\n",
    "example if your student ID is 12345678, then the submission should be 12345678.zip.\n",
    "2. You can only use python. You should work with our provided .ipynb file and  submit your solution as a jupyter notebook.\n",
    "3. <font color=red>Make sure all the data files needed to run your code is within the folder and loaded with\n",
    "relative path. We should be able to run your code without making any modifications</font>.\n",
    "\n",
    "## Instruction for report submission\n",
    "1. You report should be brief and to the point. When asked for comments, your comment\n",
    "should not be more than 3-4 lines.\n",
    "2. Do not include your code in the report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
